
04.03.2022

issues with lean attention:
under jit script:
1) adding dropout even if dropout_prob==0
2) attention_scores & attention_probs both are nonempty in ctx

task:
1) view non-contiguous buffers how are they handled in apex attention kernel 
2) try to use apex softmax scaled masked kernel instead of standard one,
   as standard one is memory inefficient (tmp var)
3) try to implement in python attention with query blocks

07.03.2022
1) no explicit logic in apex cuda-attention for making buffer contiguous
   cublasGemmEx takes the storage pointer as it is (
   storage offset is took into account but not stride)
2) scaled_masked_softmax_cuda from apex applied to lean,
   reduced substantially memory on softmax backward (no tmp var, immediate free ctx.softmax_results). 
   1 in ctx, 1 - output grad, softmax in place ~ 2
   on forward - allocate (1 on ret, 1 from mult) ~ 2


18.04.2022:
fmha
fwd:bwd (2048x10)

py lean: 11ms:25ms

fwd-block=256 bwd-block=256
6:19ms

no first block fmha (incorrect logic)
6:13ms

fwd-block=512 bwd-block=256
4.6:19ms

fwd-block=384 bwd-block=384
5:13ms

